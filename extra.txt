        # property_tobe_verfied='Pmax=?[!(("a3" & "p3") | ("a5" & "p5") | ("a7" & "p7") | ("a3" & "h3") | ("a5" & "h5") | ("a7" & "h7")) U "goal"]'
        # property_tobe_verfied='Pmax=?[(!(("a3" & "p3") | ("a5" & "p5") | ("a7" & "p7")| ("a3" & "h3") | ("a5" & "h5") | ("a7" & "h7"))) U "goal"]'
        # property_tobe_verfied='Pmax=?[(!(("a3" & "p3") | ("a5" & "p5") | ("a7" & "p7"))) U "goal"]'
        # property_tobe_verfied='Pmax=?[(!(("a3" & "p3") | ("a3" & "h3") | ("a5" & "h5") | ("a7" & "h7"))) U "goal"]'
        


                # formula_human='Pmax=?[(!(("h3" & "p3") | ("h5" & "p5") | ("h7" & "p7")| ("a3" & "h3") | ("a5" & "h5") | ("a7" & "h7"))) U "goal2"]'
        # formula_human='Pmax=?[(!(("h3" & "p3") | ("a3" & "h3") | ("a5" & "h5") | ("a7" & "h7"))) U "goal2"]'



    index=[i for i in range(0, len(data_out))]
    fig = plt.figure()
    ax = plt.axes()
    xline = []
    yline = []
    for data in data_out: 
       xline.append(data[1])
       yline.append(data_out[data])
    

    plt.plot(index, xline, label="prob. based on belief model", linestyle="-")
    plt.plot(index, yline, label="prob. after applying policies", linestyle="--")
    plt.legend()
    plt.show()

    # name='porbability'+str(int(time.time()))+'.png'

    # # plt.savefig(name)



    # def get_policies(analyzer, model, result, actions):
    #     Agent1_pol = dict()
    #     Agent2_pol = dict()

    #     move_ids = [m_s.id for m_s in model.states for s_i in m_s.labels if 'go' in s_i or 'stop' in s_i]

    #     action_points = set(range(model.nr_states)) - set(move_ids)
    #     actions1 = actions
    #     actions2 = ['go2','stop2']

    #     for s_i in action_points:
    #         # print(model.states[s_i].labels)

    #         for l_i in model.states[s_i].labels:
    #             if 'a' in l_i:
    #                 s_state = l_i
    #             elif 'h' in l_i:
    #                 x_state = l_i
    #             elif 'p' in l_i:
    #                 p_state = l_i

    #         deterministic_choice=result.scheduler.get_choice(s_i).get_deterministic_choice()
    #         transition_at_s_i = model.states[s_i].actions[deterministic_choice].transitions
    #         hold_state = model.states[int(re.findall('\\d+',str(transition_at_s_i))[0])]
    #         # print(model.states[s_i].labels)
    #         next_action = result.scheduler.get_choice(hold_state).get_deterministic_choice()
    #         next_state = model.states[int(re.findall('\\d+', str(hold_state.actions[int(next_action)].transitions))[0])]
    #         # print(next_state)
    #         # if 'crash' not in next_state.labels and 'goal' not in next_state.labels:
    #         text= not analyzer.is_crush_exist(next_state.labels)
    #         # print(text)
    #         if not analyzer.is_crush_exist(next_state.labels) and 'goal' not in next_state.labels:
    #             act_tup = tuple()
    #             act_tuple1=[l_ind for l_ind,l_a in enumerate(actions1) if l_a in next_state.labels]
    #             # act_tuple2=[l_ind for l_ind,l_a in enumerate(actions2) if l_a in next_state.labels]
    #             if len(act_tuple1)>0 :
    #                 act_tup += (act_tuple1[0],)
    #                 # act_tup += (act_tuple2[0],)
    #                 Agent1_pol.update({(s_state, x_state, p_state): act_tup[0] })
    #     return Agent1_pol
